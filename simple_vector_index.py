#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆå‘é‡ç´¢å¼•ç³»ç»Ÿ - é¿å…å¤æ‚ä¾èµ–
"""

import os
import json
import hashlib
import sqlite3
import pickle
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import argparse

# åŸºç¡€åº“
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# æ–‡æ¡£å¤„ç†
try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

try:
    import docx
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

try:
    from PIL import Image
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

class SimpleVectorIndex:
    def __init__(self, downloads_dir: str = "./documents", 
                 index_dir: str = "./.vector_index"):
        """ç®€åŒ–ç‰ˆå‘é‡ç´¢å¼•ç³»ç»Ÿ"""
        self.downloads_dir = Path(downloads_dir).expanduser()
        self.index_dir = Path(index_dir).expanduser()
        self.index_dir.mkdir(exist_ok=True)
        
        # ä½¿ç”¨ TF-IDF å‘é‡åŒ–ï¼Œé’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words=None,  # ä¸ä½¿ç”¨è‹±æ–‡åœç”¨è¯ï¼Œæ”¯æŒä¸­æ–‡
            ngram_range=(1, 3),  # æ‰©å±•åˆ°3-gramï¼Œæ›´å¥½æ”¯æŒä¸­æ–‡è¯æ±‡
            min_df=1,
            max_df=0.95,
            token_pattern=r'[\u4e00-\u9fff]+|[a-zA-Z0-9]+',  # æ”¯æŒä¸­æ–‡å­—ç¬¦å’Œè‹±æ–‡æ•°å­—
            analyzer='char',  # ä½¿ç”¨å­—ç¬¦çº§åˆ†æï¼Œæ›´é€‚åˆä¸­æ–‡
        )
        
        # æ–‡æ¡£å†…å®¹å’Œå‘é‡
        self.documents = []
        self.document_vectors = None
        
        # æ•°æ®åº“
        self.db_path = self.index_dir / "metadata.db"
        self._init_database()
        
        print(f"ç®€åŒ–ç‰ˆå‘é‡ç´¢å¼•ç³»ç»Ÿå·²åˆå§‹åŒ–")
        print(f"æ–‡æ¡£ç›®å½•: {self.downloads_dir}")
        print(f"ç´¢å¼•ç›®å½•: {self.index_dir}")
        
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS file_metadata (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_path TEXT UNIQUE NOT NULL,
                file_name TEXT NOT NULL,
                file_size INTEGER,
                file_type TEXT,
                last_modified TIMESTAMP,
                content_hash TEXT,
                content_text TEXT,
                vector_index INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        conn.commit()
        conn.close()
    
    def _extract_text_from_pdf(self, file_path: Path) -> str:
        """ä»PDFæå–æ–‡æœ¬"""
        if not PDF_AVAILABLE:
            return f"PDFæ–‡ä»¶: {file_path.name} (éœ€è¦å®‰è£…PyPDF2)"
        
        try:
            with open(file_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                return text.strip()
        except Exception as e:
            return f"PDFæ–‡ä»¶: {file_path.name} (è¯»å–å¤±è´¥: {str(e)})"
    
    def _extract_text_from_docx(self, file_path: Path) -> str:
        """ä»Wordæ–‡æ¡£æå–æ–‡æœ¬"""
        if not DOCX_AVAILABLE:
            return f"Wordæ–‡æ¡£: {file_path.name} (éœ€è¦å®‰è£…python-docx)"
        
        try:
            doc = docx.Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text.strip()
        except Exception as e:
            return f"Wordæ–‡æ¡£: {file_path.name} (è¯»å–å¤±è´¥: {str(e)})"
    
    def _extract_text_from_image(self, file_path: Path) -> str:
        """ä»å›¾ç‰‡æå–æ–‡æœ¬"""
        if not OCR_AVAILABLE:
            return f"å›¾ç‰‡æ–‡ä»¶: {file_path.name} (éœ€è¦å®‰è£…PILå’Œpytesseract)"
        
        try:
            image = Image.open(file_path)
            text = pytesseract.image_to_string(image, lang='chi_sim+eng')
            return f"å›¾ç‰‡æ–‡ä»¶: {file_path.name}\næå–æ–‡æœ¬: {text}"
        except Exception as e:
            return f"å›¾ç‰‡æ–‡ä»¶: {file_path.name} (OCRå¤±è´¥: {str(e)})"
    
    def _extract_text_from_csv(self, file_path: Path) -> str:
        """ä»CSVæå–ä¿¡æ¯"""
        if not PANDAS_AVAILABLE:
            return f"CSVæ–‡ä»¶: {file_path.name} (éœ€è¦å®‰è£…pandas)"
        
        try:
            df = pd.read_csv(file_path, nrows=50)
            info = f"CSVæ–‡ä»¶: {file_path.name}\n"
            info += f"åˆ—å: {', '.join(df.columns.tolist())}\n"
            info += f"è¡Œæ•°: {len(df)}\n"
            info += f"æ•°æ®é¢„è§ˆ:\n{df.head().to_string()}"
            return info
        except Exception as e:
            return f"CSVæ–‡ä»¶: {file_path.name} (è¯»å–å¤±è´¥: {str(e)})"
    
    def _extract_text_from_file(self, file_path: Path) -> str:
        """ä»æ–‡ä»¶æå–æ–‡æœ¬å†…å®¹"""
        file_ext = file_path.suffix.lower()
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†
        if file_ext == '.pdf':
            content = self._extract_text_from_pdf(file_path)
        elif file_ext in ['.docx', '.doc']:
            content = self._extract_text_from_docx(file_path)
        elif file_ext in ['.jpg', '.jpeg', '.png', '.bmp']:
            content = self._extract_text_from_image(file_path)
        elif file_ext == '.csv':
            content = self._extract_text_from_csv(file_path)
        elif file_ext in ['.txt', '.md', '.py', '.js', '.json', '.xml', '.html']:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
            except Exception as e:
                content = f"æ–‡æœ¬æ–‡ä»¶: {file_path.name} (è¯»å–å¤±è´¥: {str(e)})"
        else:
            # é»˜è®¤å¤„ç†
            content = f"æ–‡ä»¶: {file_path.name}\nç±»å‹: {file_ext}\nå¤§å°: {file_path.stat().st_size} bytes"
        
        # æ·»åŠ æ–‡ä»¶è·¯å¾„ä¿¡æ¯
        full_content = f"æ–‡ä»¶è·¯å¾„: {file_path}\næ–‡ä»¶å: {file_path.name}\n{content}"
        return full_content
    
    def build_index(self, max_files: int = 500):
        """æ„å»ºç´¢å¼•"""
        print("å¼€å§‹æ„å»ºç´¢å¼•...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ¸…ç©ºæ—§æ•°æ®
        cursor.execute("DELETE FROM file_metadata")
        self.documents = []
        
        file_count = 0
        processed_count = 0
        
        # éå†æ–‡ä»¶
        for file_path in self.downloads_dir.rglob('*'):
            if file_path.is_file() and processed_count < max_files:
                file_count += 1
                
                # è·³è¿‡éšè—æ–‡ä»¶å’Œç´¢å¼•ç›®å½•
                if file_path.name.startswith('.') or '.vector_index' in str(file_path):
                    continue
                
                try:
                    # æå–æ–‡æœ¬å†…å®¹
                    content = self._extract_text_from_file(file_path)
                    if not content.strip():
                        continue
                    
                    # æ·»åŠ åˆ°æ–‡æ¡£åˆ—è¡¨
                    self.documents.append(content)
                    vector_index = len(self.documents) - 1
                    
                    # ä¿å­˜å…ƒæ•°æ®
                    file_stat = file_path.stat()
                    cursor.execute("""
                        INSERT INTO file_metadata 
                        (file_path, file_name, file_size, file_type, last_modified, 
                         content_text, vector_index)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        str(file_path),
                        file_path.name,
                        file_stat.st_size,
                        file_path.suffix.lower(),
                        datetime.fromtimestamp(file_stat.st_mtime),
                        content[:2000],  # åªå­˜å‚¨å‰2000å­—ç¬¦
                        vector_index
                    ))
                    
                    processed_count += 1
                    if processed_count % 50 == 0:
                        print(f"å·²å¤„ç† {processed_count} ä¸ªæ–‡ä»¶...")
                        
                except Exception as e:
                    print(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    continue
        
        if self.documents:
            # æ„å»º TF-IDF å‘é‡
            print("æ­£åœ¨æ„å»ºTF-IDFå‘é‡...")
            self.document_vectors = self.vectorizer.fit_transform(self.documents)
            
            # ä¿å­˜å‘é‡åŒ–å™¨å’Œæ–‡æ¡£
            data_to_save = {
                'vectorizer': self.vectorizer,
                'documents': self.documents,
                'document_vectors': self.document_vectors
            }
            
            with open(self.index_dir / "vectors.pkl", 'wb') as f:
                pickle.dump(data_to_save, f)
        
        conn.commit()
        conn.close()
        
        print(f"ç´¢å¼•æ„å»ºå®Œæˆï¼")
        print(f"æ€»æ–‡ä»¶æ•°: {file_count}")
        print(f"æˆåŠŸå¤„ç†: {processed_count}")
        print(f"å‘é‡ç»´åº¦: {self.document_vectors.shape if self.document_vectors is not None else 0}")
    
    def load_index(self):
        """åŠ è½½ç´¢å¼•"""
        vectors_path = self.index_dir / "vectors.pkl"
        if vectors_path.exists():
            with open(vectors_path, 'rb') as f:
                data = pickle.load(f)
                self.vectorizer = data['vectorizer']
                self.documents = data['documents']
                self.document_vectors = data['document_vectors']
            print(f"å·²åŠ è½½ç´¢å¼•ï¼ŒåŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£")
        else:
            print("æœªæ‰¾åˆ°å·²ä¿å­˜çš„ç´¢å¼•ï¼Œè¯·å…ˆè¿è¡Œ build_index()")
    
    def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """æœç´¢ - ç»“åˆå‘é‡ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…"""
        if not self.documents:
            print("ç´¢å¼•ä¸ºç©ºï¼Œè¯·å…ˆæ„å»ºç´¢å¼•")
            return []
        
        # å‘é‡åŒ–æŸ¥è¯¢
        query_vector = self.vectorizer.transform([query])
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vector, self.document_vectors)[0]
        
        # è·å–è¯¦ç»†ä¿¡æ¯
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åŒæ—¶è¿›è¡Œæ•°æ®åº“æ–‡æœ¬æœç´¢ä½œä¸ºè¡¥å……
        cursor.execute("""
            SELECT file_path, file_name, file_type, file_size, 
                   last_modified, content_text, vector_index
            FROM file_metadata 
            WHERE content_text LIKE ?
        """, (f'%{query}%',))
        
        text_matches = cursor.fetchall()
        
        # åˆå¹¶ç»“æœ
        results = []
        processed_indices = set()
        
        # å…ˆæ·»åŠ å‘é‡æœç´¢ç»“æœ
        top_indices = np.argsort(similarities)[::-1][:top_k*2]  # æ‰©å±•æœç´¢èŒƒå›´
        
        for idx in top_indices:
            score = similarities[idx]
            if score > 0:
                cursor.execute("""
                    SELECT file_path, file_name, file_type, file_size, 
                           last_modified, content_text 
                    FROM file_metadata 
                    WHERE vector_index = ?
                """, (int(idx),))
                
                row = cursor.fetchone()
                if row:
                    results.append({
                        'score': float(score),
                        'file_path': row[0],
                        'file_name': row[1],
                        'file_type': row[2],
                        'file_size': row[3],
                        'last_modified': row[4],
                        'content_preview': row[5][:300] + "..." if len(row[5]) > 300 else row[5],
                        'match_type': 'vector'
                    })
                    processed_indices.add(idx)
        
        # æ·»åŠ æ–‡æœ¬åŒ¹é…ç»“æœï¼ˆå¦‚æœå‘é‡æœç´¢æ²¡æœ‰æ‰¾åˆ°ï¼‰
        for row in text_matches:
            vector_idx = row[6]
            if vector_idx not in processed_indices:
                # ä¸ºæ–‡æœ¬åŒ¹é…è®¾ç½®è¾ƒé«˜çš„åŸºç¡€åˆ†æ•°
                text_score = 0.8  
                results.append({
                    'score': text_score,
                    'file_path': row[0],
                    'file_name': row[1],
                    'file_type': row[2],
                    'file_size': row[3],
                    'last_modified': row[4],
                    'content_preview': row[5][:300] + "..." if len(row[5]) > 300 else row[5],
                    'match_type': 'text'
                })
        
        # æŒ‰åˆ†æ•°é‡æ–°æ’åºå¹¶è¿”å›top-k
        results.sort(key=lambda x: x['score'], reverse=True)
        
        conn.close()
        return results[:top_k]
    
    def search_cli(self, query: str, top_k: int = 10):
        """å‘½ä»¤è¡Œæœç´¢"""
        print(f"\nğŸ” æœç´¢: '{query}'")
        print("=" * 60)
        
        results = self.search(query, top_k)
        
        if not results:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
            return
        
        for i, result in enumerate(results, 1):
            match_type = result.get('match_type', 'vector')
            match_icon = "ğŸ¯" if match_type == 'text' else "ğŸ“Š"
            print(f"\nğŸ“„ ç»“æœ {i} {match_icon} (åˆ†æ•°: {result['score']:.3f})")
            print(f"   æ–‡ä»¶: {result['file_name']}")
            print(f"   è·¯å¾„: {result['file_path']}")
            print(f"   ç±»å‹: {result['file_type']}")
            print(f"   å¤§å°: {result['file_size']} bytes")
            print(f"   ä¿®æ”¹æ—¶é—´: {result['last_modified']}")
            print(f"   åŒ¹é…ç±»å‹: {'æ–‡æœ¬åŒ¹é…' if match_type == 'text' else 'å‘é‡ç›¸ä¼¼åº¦'}")
            print(f"   å†…å®¹é¢„è§ˆ: {result['content_preview']}")
            print("-" * 60)

def main():
    parser = argparse.ArgumentParser(description="ç®€åŒ–ç‰ˆæ–‡æ¡£å‘é‡ç´¢å¼•")
    parser.add_argument("--build", action="store_true", help="æ„å»ºç´¢å¼•")
    parser.add_argument("--search", type=str, help="æœç´¢æŸ¥è¯¢")
    parser.add_argument("--top_k", type=int, default=10, help="è¿”å›ç»“æœæ•°é‡")
    parser.add_argument("--max_files", type=int, default=500, help="æœ€å¤§å¤„ç†æ–‡ä»¶æ•°")
    parser.add_argument("--documents_dir", type=str, default="./documents", help="æ–‡æ¡£ç›®å½•è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    system = SimpleVectorIndex(downloads_dir=args.documents_dir)
    
    if args.build:
        system.build_index(args.max_files)
    elif args.search:
        system.load_index()
        system.search_cli(args.search, args.top_k)
    else:
        # äº¤äº’å¼æ¨¡å¼
        system.load_index()
        print("\nğŸš€ ç®€åŒ–ç‰ˆå‘é‡ç´¢å¼•ç³»ç»Ÿå·²å¯åŠ¨ï¼")
        print("è¾“å…¥æŸ¥è¯¢å†…å®¹è¿›è¡Œæœç´¢ï¼Œè¾“å…¥ 'quit' é€€å‡º")
        
        while True:
            query = input("\nğŸ” è¯·è¾“å…¥æœç´¢æŸ¥è¯¢: ").strip()
            if query.lower() in ['quit', 'exit', 'q']:
                break
            if query:
                system.search_cli(query)

if __name__ == "__main__":
    main()